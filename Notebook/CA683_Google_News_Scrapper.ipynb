{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google News Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code is used to scrape news articles by searching on http://www.news.google.com with keywords of interest and specific date ranges. We generate a csv files of news text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from newspaper3k) (8.0.1)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from newspaper3k) (5.3.1)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.2-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from newspaper3k) (2.24.0)\n",
      "Collecting cssselect>=0.9.2\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from newspaper3k) (4.6.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from newspaper3k) (3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from newspaper3k) (2.8.1)\n",
      "Collecting tldextract>=2.0.1\n",
      "  Downloading tldextract-3.1.0-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in ./Anaconda/anaconda3/lib/python3.8/site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in ./Anaconda/anaconda3/lib/python3.8/site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.0.1)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from requests>=2.10.0->newspaper3k) (1.25.11)\n",
      "Requirement already satisfied: click in ./Anaconda/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: regex in ./Anaconda/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (2020.10.15)\n",
      "Requirement already satisfied: tqdm in ./Anaconda/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (4.50.2)\n",
      "Requirement already satisfied: joblib in ./Anaconda/anaconda3/lib/python3.8/site-packages (from nltk>=3.2.1->newspaper3k) (0.17.0)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in ./Anaconda/anaconda3/lib/python3.8/site-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
      "Building wheels for collected packages: feedfinder2, jieba3k, tinysegmenter, sgmllib3k\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=7025af686ab2faaff7bf0b29424dd3b9daf182171a75f4664323e01fadbb5c63\n",
      "  Stored in directory: /Users/hitman94/Library/Caches/pip/wheels/b6/09/68/a9f15498ac02c23dde29f18745bc6a6f574ba4ab41861a3575\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398407 sha256=086e04880362a168846ec07efd6591be6c8774c208f2b455e3ff355af1316816\n",
      "  Stored in directory: /Users/hitman94/Library/Caches/pip/wheels/1f/7e/0c/54f3b0f5164278677899f2db08f2b07943ce2d024a3c862afb\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13536 sha256=32abd67bc7334257f29ad74963533d6f4e14233e50eb9a524d0e152ab73aa065\n",
      "  Stored in directory: /Users/hitman94/Library/Caches/pip/wheels/99/74/83/8fac1c8d9c648cfabebbbffe97a889f6624817f3aa0bbe6c09\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=7ee109974443b6a3e6eebeb08f29c057cb1d84bb800442e5a1e396bbbaff2705\n",
      "  Stored in directory: /Users/hitman94/Library/Caches/pip/wheels/83/63/2f/117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built feedfinder2 jieba3k tinysegmenter sgmllib3k\n",
      "Installing collected packages: feedfinder2, jieba3k, tinysegmenter, sgmllib3k, feedparser, cssselect, requests-file, tldextract, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Config\n",
    "import nltk\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL below searches google news based on two keywords \"bitcoin\" and \"cryptocurrency\"\n",
    "URL = \"https://www.google.com/search?q=bitcoin+cryptocurrency&hl=en&gl=us&as_drrb=b&tbas=0&tbs=cdr:1,cd_min:{minDate},cd_max:{maxDate},sbd:1&tbm=nws&sxsrf=ACYBGNRfmviSo9arK1e_P_YIl5wsskZBPw:1574225634362&source=lnt&sa=X&ved=0ahUKEwj4wu29__flAhWV9Z4KHaKJAGcQpwUIIA&biw=1685&bih=863&dpr=1.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hitman94/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#config will allow us to access the specified url for which we are not authorized. Sometimes we may get 403 \n",
    "#client error while parsing #the link to download the article.\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\"\"\"headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
    "    'Content-Type': 'text/html',\n",
    "}\"\"\"\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxCount = 9  # max 10 news articles per day\n",
    "dateFormat = '%m/%d/%Y'\n",
    "newsColumns = ['index', 'date', 'news_1_url', 'news_1_text',\n",
    "             'news_2_url', 'news_2_text', 'news_3_url', 'news_3_text',\n",
    "             'news_4_url', 'news_4_text', 'news_5_url', 'news_5_text',\n",
    "             'news_6_url', 'news_6_text', 'news_7_url', 'news_7_text',\n",
    "             'news_8_url', 'news_8_text', 'news_9_url', 'news_9_text',\n",
    "             'news_10_url', 'news_10_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGoogleNewsScrapper(**params):\n",
    "    outputFileName = ''\n",
    "    for key, value in params.items():\n",
    "        if key == 'minDate':\n",
    "            minDate = value\n",
    "        if key == 'outputFile':\n",
    "            outputFileName = value\n",
    "    \n",
    "    googlenews=GoogleNews(start=minDate,end=minDate)\n",
    "    googlenews.search('Bitcoin Cryptocurrency')\n",
    "    result=googlenews.result()\n",
    "    df=pd.DataFrame(result)\n",
    "\n",
    "    newsDataDictionary = dict() #creating a new dictionary\n",
    "    columns = []\n",
    "    newsDataDictionary['date'] = minDate\n",
    "    columns.append('date')\n",
    "    \"\"\"response = requests.get(URL.format(**params), headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    newsDataDictionary['status_code'] = response.status_code\n",
    "    columns.append('status_code')\n",
    "    if response.status_code != 200:\n",
    "        print(\"******** fail ********** \")\n",
    "        return\"\"\"\n",
    "    # print(response.url)\n",
    "    #newsDataDictionary['url'] = response.url\n",
    "    #columns.append('url')\n",
    "    \n",
    "    #listofLinkStr = []\n",
    "    count = 1\n",
    "   \n",
    "    \"\"\"for link in soup.find_all('a'):\n",
    "        linkStr = str(link.get('href'))\n",
    "        try:\n",
    "            #linkStr.startswith(\"https://\") and linkStr.find('google.com') == -1 and linkStr.find(\n",
    "                    #\"https://www.youtube.com/\") == -1 and linkStr.find(\"https://wwwblogger..com/\") == -1\n",
    "            if linkStr.startswith(\"/url?q=https://\"):\n",
    "                \n",
    "                #linkStrConverisonToList = list(linkStr)\n",
    "                \n",
    "                #code to extraxt href link from the soup instance\n",
    "                linkStrList = linkStr.split('&')\n",
    "                slicedList = list(linkStrList[0])\n",
    "                del slicedList[:7]\n",
    "                #del slicedList[-4:]\n",
    "                listToStr = ' '.join(map(str, slicedList)) \n",
    "                linkStr = listToStr.replace(\" \",\"\")\n",
    "                \n",
    "                if linkStr in listofLinkStr:\n",
    "                    print(\"\")\n",
    "                else:\n",
    "                    listofLinkStr.append(linkStr)\n",
    "\n",
    "                article = Article(linkStr)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                print(linkStr)\n",
    "                #print(article.authors)\n",
    "                #print(article.publish_date)\n",
    "                #print(article.text)\n",
    "                newsCount = 'news_' + str(count)\n",
    "                newsDataDictionary[newsCount + '_url'] = linkStr\n",
    "                newsDataDictionary[newsCount + '_text'] = article.text\n",
    "                newsDataDictionary[newsCount + '_publish_date'] = article.publish_date\n",
    "\n",
    "                columns.append(newsCount + '_url')\n",
    "                columns.append(newsCount + '_text')\n",
    "                columns.append(newsCount + '_publish_date')\n",
    "                count += 1\n",
    "                if count >= maxCount:\n",
    "                    break\n",
    "        except:\n",
    "            pass\"\"\"\n",
    "        \n",
    "    \"\"\"print(\"----------------------------------------------------------------------------------------\")\n",
    "    for x in range(len(listofLinkStr)): \n",
    "        print(listofLinkStr[x]) \n",
    "    print(\"----------------------------------------------------------------------------------------\")\"\"\"\n",
    "    \n",
    "    for ind in df.index:\n",
    "        article = Article(df['link'][ind],config=config)\n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "        except:\n",
    "            print('***FAILED TO DOWNLOAD***', article.url)\n",
    "            continue\n",
    "        article.nlp()\n",
    "        newsCount = 'news_' + str(count)\n",
    "        newsDataDictionary[newsCount + '_url'] = df['link'][ind]\n",
    "        newsDataDictionary[newsCount + '_title']=article.title\n",
    "        newsDataDictionary[newsCount + '_text']=article.text\n",
    "        #list.append(dict)\n",
    "        \n",
    "        columns.append(newsCount + '_url')\n",
    "        columns.append(newsCount + '_title')\n",
    "        columns.append(newsCount + '_text')\n",
    "        \n",
    "\n",
    "        count += 1\n",
    "        if count >= maxCount:\n",
    "            break\n",
    "\n",
    "    #news_df=pd.DataFrame(list)\n",
    "    #news_df.to_csv(\"/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/articles.csv\")\n",
    "    \n",
    "    \"\"\"ctr = 1\n",
    "    for linkStr in listofLinkStr:\n",
    "        article = Article(linkStr)\n",
    "        \n",
    "        try:\n",
    "            article.download()\n",
    "            article.parse()\n",
    "        except:\n",
    "            print('***FAILED TO DOWNLOAD***', article.url)\n",
    "            continue\n",
    "            \n",
    "        #article.download()\n",
    "        #article.parse()\n",
    "        \n",
    "        #print(article.authors)\n",
    "        #print(article.publish_date)\n",
    "        #print(article.text)\n",
    "        newsCount = 'news_' + str(ctr)\n",
    "        newsDataDictionary[newsCount + '_url'] = linkStr\n",
    "        newsDataDictionary[newsCount + '_text'] = article.text\n",
    "        #newsDataDictionary[newsCount + '_publish_date'] = article.publish_date\n",
    "\n",
    "        columns.append(newsCount + '_url')\n",
    "        columns.append(newsCount + '_text')\n",
    "        #columns.append(newsCount + '_publish_date') \n",
    "        \n",
    "        ctr += 1\n",
    "        if ctr >= maxCount:\n",
    "                break\"\"\"\n",
    "    \n",
    "    newsDataDf = pd.DataFrame(newsDataDictionary, index=[0],columns=newsColumns)\n",
    "    \n",
    "    \"\"\"\n",
    "    if os.path.exists('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + outputFileName):\n",
    "        keep_header = False\n",
    "        news_data_df.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + outputFileName, mode='a', header=keep_header)\n",
    "    else:\n",
    "        keep_header = True\n",
    "        news_data_df.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + outputFileName)\"\"\"\n",
    "        \n",
    "    # if file does not exist write header \n",
    "    if not os.path.isfile('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + outputFileName):\n",
    "        newsDataDf.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + outputFileName)\n",
    "    else: # else it exists so append without writing the header\n",
    "        newsDataDf.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + outputFileName, mode='a', header=False)\n",
    "\n",
    "    \n",
    "    #news_data_df.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + outputFileName)\n",
    "    return newsDataDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no issues in this code\n",
    "def googleNewsScrapper(startDate, endDate, outputFileName):\n",
    "    stepObj = datetime.timedelta(days=1)\n",
    "    startDateTimeObj = datetime.datetime.strptime(startDate, dateFormat)\n",
    "    endDateTimeObj = datetime.datetime.strptime(endDate, dateFormat)\n",
    "\n",
    "    while startDateTimeObj <= endDateTimeObj:\n",
    "        startDate = startDateTimeObj.strftime(dateFormat)\n",
    "        print(startDate)\n",
    "        runGoogleNewsScrapper(minDate=startDate, maxDate=startDate, outputFile=outputFileName)\n",
    "        time.sleep(np.random.randint(2, 5))\n",
    "        startDateTimeObj += stepObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortNewsReport(inputFileName, cleanedOutputFileName, saveIndex=False):\n",
    "    df = pd.read_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + inputFileName)\n",
    "    df = df.set_index('date', drop=True)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.sort_index().drop_duplicates(keep='first')\n",
    "    df.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + cleanedOutputFileName)\n",
    "    if saveIndex:\n",
    "        df_i = pd.DataFrame(df.index)\n",
    "        df_i.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + cleanedOutputFileName[0:-4] + '_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanNewsReport(inputFileName, cleanedOutputFileName, saveIndex=False):\n",
    "    ctr = 1\n",
    "    final_df = pd.read_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + inputFileName)\n",
    "    \n",
    "    # get only given columns\n",
    "    final_df = final_df[['date', 'news_1_text', 'news_2_text', 'news_3_text', 'news_4_text', 'news_5_text', 'news_6_text',\n",
    "             'news_7_text', 'news_8_text', 'news_9_text', 'news_10_text']]\n",
    "    final_df = final_df.set_index('date', drop=True)\n",
    "    final_df.index = pd.to_datetime(final_df.index, format=dateFormat)\n",
    "    # sort and drop duplicates\n",
    "    final_df = final_df.sort_index().drop_duplicates(keep='first')\n",
    "    index = np.unique(final_df.index, return_index=True)[1]\n",
    "    final_df = final_df.iloc[index]\n",
    "    final_df.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + cleanedOutputFileName)\n",
    "    if saveIndex:\n",
    "        df_i = pd.DataFrame(final_df.index)\n",
    "        df_i.to_csv('/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/' + cleanedOutputFileName[0:-4] + '_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/01/2021\n",
      "03/02/2021\n",
      "***FAILED TO DOWNLOAD*** https://www.nasdaq.com/articles/what-is-bitcoin-and-how-does-it-work-2021-03-02\n",
      "03/03/2021\n",
      "03/04/2021\n",
      "***FAILED TO DOWNLOAD*** https://money.usnews.com/investing/cryptocurrency/articles/why-everyone-should-own-some-cryptocurrency\n",
      "***FAILED TO DOWNLOAD*** https://seekingalpha.com/article/4411554-case-against-cryptocurrencies\n",
      "03/05/2021\n",
      "***FAILED TO DOWNLOAD*** https://www.washingtonpost.com/business/2021/03/05/john-mcafee-charged-cryptocurrency/\n",
      "03/06/2021\n",
      "03/07/2021\n",
      "03/08/2021\n",
      "***FAILED TO DOWNLOAD*** https://seekingalpha.com/news/3670206-bitcoin-roundup-back-over-50k-post-stimulus-norways-aker-wants-in\n",
      "03/09/2021\n",
      "03/10/2021\n",
      "03/11/2021\n",
      "03/12/2021\n",
      "***FAILED TO DOWNLOAD*** https://mooresvilletribune.com/business/investment/personal-finance/will-buying-bitcoin-impact-my-tax-return/article_f188a792-2819-556c-b762-fe1415aedc87.html\n",
      "03/13/2021\n",
      "***FAILED TO DOWNLOAD*** https://www.bangkokpost.com/business/2083083/bitcoin-surging-toward-60-000\n",
      "03/14/2021\n",
      "03/15/2021\n",
      "03/16/2021\n",
      "03/17/2021\n",
      "03/18/2021\n",
      "03/19/2021\n",
      "03/20/2021\n",
      "***FAILED TO DOWNLOAD*** https://www.nasdaq.com/articles/top-cryptocurrencies-to-buy-now-4-to-watch-this-week-2021-03-20\n"
     ]
    }
   ],
   "source": [
    "#no issues in this code\n",
    "if __name__ == \"__main__\":\n",
    "    startDate = '03/01/2021'\n",
    "    # endDate = datetime.datetime.now().strftime(dateFormat)\n",
    "    endDate = '03/20/2021'\n",
    "    newsFilename = 'google_news_final_test.csv'\n",
    "\n",
    "    googleNewsScrapper(startDate, endDate, newsFilename)\n",
    "    newsCleanedFilename = newsFilename[0:-4] + '_cleaned.csv'\n",
    "    cleanNewsReport(newsFilename, newsCleanedFilename)\n",
    "    #testFunction(minDate = startDate, maxDate = endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 4 7 9]\n",
      "   Column_Name\n",
      "1            2\n",
      "0            5\n",
      "2            6\n",
      "4            7\n",
      "7            8\n",
      "9            9\n"
     ]
    }
   ],
   "source": [
    "#testing code\n",
    "from pandas import DataFrame\n",
    "final_df_list = [5,2,6,2,7,5,6,8,2,9]\n",
    "test_index = np.unique(final_df_list, return_index=True)[1]\n",
    "print(test_index)\n",
    "df = DataFrame(final_df_list,columns=['Column_Name'])\n",
    "df = df.iloc[test_index]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing code\n",
    "def testFunction(**params):\n",
    "    response = requests.get(URL.format(**params), headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_csv(\"/Volumes/Sashank Work/MSc DCU/Semester 2/CA683 Data Analytics and Data Mining/Assignment/Dataset/Google News/google_news_final_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of          date                                        news_1_text  \\\n",
       "0  2021-03-18  eToro's CEO speculates on what’s driving the c...   \n",
       "\n",
       "                                         news_2_text  \\\n",
       "0  SINGAPORE: Amid Singapore's push for sustainab...   \n",
       "\n",
       "                                         news_3_text  \\\n",
       "0  Join legendary investor Matt McCall on March 2...   \n",
       "\n",
       "                                         news_4_text  \\\n",
       "0  (REUTERS) - Accounting for and storing highly ...   \n",
       "\n",
       "                                         news_5_text  news_6_text  \\\n",
       "0  How bitcoin is consider the magnetic cryptocur...          NaN   \n",
       "\n",
       "   news_7_text  news_8_text  news_9_text  news_10_text  \n",
       "0          NaN          NaN          NaN           NaN  >"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_read.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://investorplace.com/moneywire/2021/03/bitcoin-btc-not-laughing-at-my-bold-cryptocurrency-prediction/\"\n",
    "newsDictionary = dict() #creating a new dictionary\n",
    "cutter = 1\n",
    "article = Article(link)\n",
    "article.download()\n",
    "article.parse()\n",
    "\n",
    "#print(article.authors)\n",
    "#print(article.publish_date)\n",
    "#print(article.text)\n",
    "newsCounter = 'news_' + str(cutter)\n",
    "newsDictionary[newsCounter + '_url'] = link\n",
    "newsDictionary[newsCounter + '_text'] = article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news_1_url': 'https://investorplace.com/moneywire/2021/03/bitcoin-btc-not-laughing-at-my-bold-cryptocurrency-prediction/',\n",
       " 'news_1_text': 'Join legendary investor Matt McCall on March 24 when he unveils how a new investment is set to bring Wall Street’s wealth to the everyday American.'}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
